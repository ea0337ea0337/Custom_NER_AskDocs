{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "natural-memorabilia",
   "metadata": {},
   "source": [
    "### This notebook performs the following tasks:\n",
    "### 1. Re-format Doccano entity label output to spaCy format\n",
    "### 2. Check for alignment between spaCy tokens and Doccano entitiy annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corresponding-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# other modules, code\n",
    "from html import unescape\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "import spacy\n",
    "import srsly\n",
    "from spacy import displacy\n",
    "from spacy.training import docs_to_json, offsets_to_biluo_tags, biluo_tags_to_spans\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokens import Span\n",
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intended-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_doccano(df, colname = 'data', symptom_ent = True):\n",
    "    if symptom_ent:\n",
    "        #reformat jsonl label format from doccano to import into spacy\n",
    "        df['label'] = df['label'].apply(lambda x: {'entities': [(e[0], e[1], e[2]) for e in x]})\n",
    "    else:\n",
    "        #reformat jsonl label format from doccano to import into spacy\n",
    "        df['label'] = df['label'].apply(lambda x: {'entities': [(e[0], e[1], e[2]) for e in x if e[2] != 'SYMPTOM']})\n",
    "    #create tuple from text, entities and assign to a new column.\n",
    "    df['combined'] = list(zip(df[colname], df['label']))\n",
    "    #create a list of tuples for trianing as required by spacy\n",
    "    data_list = [obs for obs in df['combined']]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generous-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_reformat_doccano(filename, colname, keep_symptom = True):\n",
    "    #read in df after annotation in doccano, exported as jsonl file\n",
    "    df_annotated = pd.read_json(filename, lines=True)\n",
    "\n",
    "    #reformat doccano annotations to spacy format as a list of tuples\n",
    "    list_annotated = reformat_doccano(df_annotated, colname, keep_symptom)\n",
    "\n",
    "    #generate lists containing raw_texts, annotations used for debugging\n",
    "    list_raw_text = [i[0] for i in list_annotated]\n",
    "    list_annotations = [i[1] for i in list_annotated]\n",
    "    \n",
    "    return [list_annotated, (list_raw_text, list_annotations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "british-despite",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_for_alignment(spacy_data_list):\n",
    "    nlp = spacy.blank('en')\n",
    "    #checking if character offsets from doccano are aligned with tokenization done by spacy\n",
    "    for row, (raw_text, entity_offsets) in enumerate(spacy_data_list):\n",
    "        print_raw_txt = False\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        example = Example.from_dict(doc, {\"entities\": entity_offsets['entities']})\n",
    "        #print (entity_offsets['entities'])\n",
    "        check_this = spacy.training.offsets_to_biluo_tags(nlp.make_doc(raw_text), entity_offsets['entities'])\n",
    "        for idx, t in enumerate(doc):\n",
    "            if check_this[idx] == '-':\n",
    "                print_raw_txt = True\n",
    "                print (t.text, t.idx, check_this[idx])\n",
    "        if print_raw_txt:\n",
    "            print (f'---Tags for example #{row}---')\n",
    "            print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "functional-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_spacy_BILOU_format(data_list, fname_prefix):\n",
    "    #Generate .jsonl files for training, eval from train_data\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    db = DocBin() #docbin object for storing data on disk\n",
    "    docs = []\n",
    "    entities_dct = {}\n",
    "    for text, annot in data_list:\n",
    "        doc = nlp(text)\n",
    "        tags = offsets_to_biluo_tags(doc, annot['entities'])\n",
    "        entities = biluo_tags_to_spans(doc, tags)\n",
    "        doc.ents = entities\n",
    "        docs.append(doc)\n",
    "        db.add(doc)\n",
    "        #loop over all the entities and group into their labeled groups for entityruler patterns\n",
    "        for ent in entities:\n",
    "            try:\n",
    "                entities_dct[ent.label_].add(ent.text.lower())\n",
    "            except:\n",
    "                entities_dct[ent.label_] = set()\n",
    "                entities_dct[ent.label_].add(ent.text.lower())\n",
    "\n",
    "    # commenting JSONL output since we can use docbin\n",
    "    # srsly.write_json(fname_prefix + '_spacy_format.json', [docs_to_json(docs)])\n",
    "    fname = fname_prefix + '.spacy'\n",
    "    db.to_disk(fname)\n",
    "    \n",
    "    return entities_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boring-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of entities present in the data\n",
    "def ent_counts(data_list):\n",
    "    ent_count_dict = {}\n",
    "    for _, annotations in data_list:\n",
    "        for ent in annotations['entities']:\n",
    "            try:\n",
    "                ent_count_dict[ent[2]] += 1\n",
    "            except:\n",
    "                ent_count_dict[ent[2]] = 1\n",
    "    return ent_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latin-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ents_to_patterns(entities_dct):\n",
    "    # generate patterns for entityruler based on entities annotated in training dataset\n",
    "    # assumes entites are LOWERCASE; generate tokens by SPLITTING ON WHITESPACE\n",
    "    patterns = []\n",
    "    for label, entities_set in entities_dct.items():\n",
    "        for ent_tokens in entities_set:\n",
    "            # Initialize a dictionary for token-patterns as describe in spacy documentation\n",
    "            label_dct = {}\n",
    "            label_dct['label'] = label\n",
    "            label_dct['pattern'] = [{'LOWER': t} for t in ent_tokens.split()]\n",
    "            patterns.append(label_dct)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-sheffield",
   "metadata": {},
   "source": [
    "### Initialize basic arguments\n",
    "TODO: Need to share the basic arguments globally across all notebooks for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heavy-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "months = ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct']\n",
    "n_start = 0\n",
    "n_end = 80 #40\n",
    "# number of months * number of examples from each month (n_end  - n_start); n_end excluded\n",
    "model_suffix = len(months) * (n_end - n_start)\n",
    "sub_group = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-accent",
   "metadata": {},
   "source": [
    "### Annotated files generated from Doccano & rehearsal data notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marine-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET this flag to False if not using SYMPTOM as part of NER model\n",
    "keep_symptom_ent = False\n",
    "\n",
    "if not keep_symptom_ent:\n",
    "    model_suffix = str(model_suffix) + '_NS_v2'\n",
    "\n",
    "# final annotated output from Doccano\n",
    "train_filename = './data/json/nsamples_480_v2_2021_6m_doccano.jsonl' #'./nsamples_240_2021_6m_doccano.jsonl'\n",
    "val_filename = './data/json/val_nsamples_240_doccano.jsonl'\n",
    "\n",
    "# annotated rehearsal filenames from generate_rehearsal_data.ipynb\n",
    "rehearsal_train_filename = './data/json/nlp_rehearsal_1000.json'\n",
    "rehearsal_val_filename = './data/json/test_nlp_rehearsal_1000.json'\n",
    "\n",
    "old_ptrns_fname = f'./data/patterns/old_patterns_240.csv' #f'./old_patterns_60.csv'\n",
    "new_ptrns_fname = f'./data/patterns/old_patterns_{model_suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-correction",
   "metadata": {},
   "source": [
    "### Read, reformat TRAIN, VAL data from Doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "muslim-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets with DISEASE entity\n",
    "train_data, (train_text, _) = import_reformat_doccano(train_filename, 'data', keep_symptom_ent)\n",
    "val_data, (val_text, _) = import_reformat_doccano(val_filename, 'data', keep_symptom_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inner-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhrsl_train_data, _ = import_reformat_doccano(rehearsal_train_filename, 'text')\n",
    "rhrsl_val_data, _ = import_reformat_doccano(rehearsal_val_filename, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-mobility",
   "metadata": {},
   "source": [
    "### Check if entities annotated in Doccano are aligned after tokenization in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "norwegian-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_alignment(train_data)\n",
    "check_for_alignment(val_data)\n",
    "check_for_alignment(rhrsl_train_data)\n",
    "check_for_alignment(rhrsl_val_data)\n",
    "#another way to check for alignment; this was recommendation from spacy warning message\n",
    "# list(zip(doc, check_this));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-hazard",
   "metadata": {},
   "source": [
    "### Convert TRAIN, VAL data into spaCy readable .JSON format, spacy binary format\n",
    "### Also generate / return sets of entities annotated for each label as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "martial-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities_dct = convert_to_spacy_BILOU_format(train_data, f'./outputs/train_n_{model_suffix}')\n",
    "_ = convert_to_spacy_BILOU_format(val_data, f'./outputs/val_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "civilian-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = convert_to_spacy_BILOU_format(rhrsl_train_data, f'./outputs/rhrsl_train_n_{model_suffix}')\n",
    "_ = convert_to_spacy_BILOU_format(rhrsl_val_data, f'./outputs/rhrsl_val_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifteen-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = convert_to_spacy_BILOU_format(train_data + rhrsl_train_data, f'./outputs/combo_train_n_{model_suffix}')\n",
    "_ = convert_to_spacy_BILOU_format(val_data + rhrsl_val_data, f'./outputs/combo_val_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "certain-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DISEASE': 748, 'DRUG': 698}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_counts(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "governing-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DISEASE': 379, 'DRUG': 271}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_counts(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "equipped-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_counts(rhrsl_train_data);\n",
    "ent_counts(rhrsl_val_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-testing",
   "metadata": {},
   "source": [
    "## ----------RULE BASED MODEL----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-wages",
   "metadata": {},
   "source": [
    "### Read in patterns from last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "trained-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in old patterns; Make sure to use the literal_eval otherwise patterns will be read-in as strings\n",
    "old_patterns_df = pd.read_csv(old_ptrns_fname, converters={'pattern':ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "helpful-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_patterns = []\n",
    "for label, pattern in old_patterns_df.itertuples(index=False):\n",
    "    dct = {}\n",
    "    if keep_symptom_ent:\n",
    "        dct = {'label': label, 'pattern': pattern}\n",
    "    else:\n",
    "        if label != 'SYMPTOM':\n",
    "            dct = {'label': label, 'pattern': pattern}\n",
    "    if (not keep_symptom_ent) and (label == 'SYMPTOM'):\n",
    "        pass\n",
    "    else:\n",
    "        old_patterns.append(dct)\n",
    "old_patterns;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-leeds",
   "metadata": {},
   "source": [
    "### Generate patterns for EntityRuler based on entity sets generated previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "finite-preserve",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patterns = ents_to_patterns(train_entities_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "funded-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DISEASE', 'DRUG'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all the labels / entity-types in this dataset\n",
    "labels = set()\n",
    "for p in patterns:\n",
    "    labels.add(p['label'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "individual-roads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplication in entity patterns\n",
    "check_duplicates = pd.DataFrame(patterns + old_patterns)\n",
    "check_duplicates['pattern'] = check_duplicates['pattern'].map(lambda x: str(x))\n",
    "check_duplicates[check_duplicates.duplicated()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-mount",
   "metadata": {},
   "source": [
    "## ----------ML Model using spacy EntityRuler----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-explorer",
   "metadata": {},
   "source": [
    "### Model 0: Blank spacy model with EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "capital-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank_rule = spacy.blank('en')\n",
    "ruler = nlp_blank_rule.add_pipe('entity_ruler')\n",
    "ruler.add_patterns(patterns + old_patterns)\n",
    "nlp_blank_rule.to_disk(f'./models/model_0_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-improvement",
   "metadata": {},
   "source": [
    "### Model 4: pre-trained spacy model with EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "manufactured-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load default spacy model with pre-trained NER\n",
    "nlp_rule_based = spacy.load('en_core_web_sm') # Model 4\n",
    "ruler = nlp_rule_based.add_pipe('entity_ruler', before='ner')\n",
    "ruler.add_patterns(patterns + old_patterns)\n",
    "nlp_rule_based.to_disk(f'./models/model_4_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-florida",
   "metadata": {},
   "source": [
    "### Model 5: pre-trained spacy mode, NER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "assisted-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "for pipe in other_pipes:\n",
    "    nlp.remove_pipe(pipe)\n",
    "nlp.pipe_names\n",
    "nlp.to_disk(f'./models/model_5_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-reducing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "measured-aberdeen",
   "metadata": {},
   "source": [
    "### Save generated patterns to disk for re-use in annotating next batch of training, val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "adopted-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_df = pd.DataFrame(patterns + old_patterns)\n",
    "patterns_df.to_csv(new_ptrns_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-adult",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
