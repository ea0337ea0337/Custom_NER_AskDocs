{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noted-combining",
   "metadata": {},
   "source": [
    "### The following models are trained in this notebook:\n",
    "### Model 0: Blank spacy model using patterns with EntityRuler\n",
    "### Model 1: pre-trained spacy model trained using training data AND revision data\n",
    "### Model 2: pre-trained spacy model trained using training data only (1 epoch, catastrophic forgetting)\n",
    "### Model 3: Blank spacy model trained using training data only\n",
    "### Model 4: pre-trained spacy model using patterns with EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corresponding-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# other modules, code\n",
    "from html import unescape\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "import spacy\n",
    "import srsly\n",
    "from spacy import displacy\n",
    "from spacy.training import docs_to_json, offsets_to_biluo_tags, biluo_tags_to_spans\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokens import Span\n",
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "instant-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_from_docbin(filename):\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    doc_bin = DocBin().from_disk(filename)\n",
    "\n",
    "    list_data = []\n",
    "    labels = set()\n",
    "    for doc in doc_bin.get_docs(nlp.vocab):\n",
    "        spans = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        list_data.append((doc.text, {'entities': spans}))\n",
    "        for ent in doc.ents:\n",
    "            labels.add(ent.label_)\n",
    "    return (list_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-mount",
   "metadata": {},
   "source": [
    "## ----------ML MODEL TRAINING----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-naples",
   "metadata": {},
   "source": [
    "### Initialize basic arguments\n",
    "TODO: Need to share the basic arguments globally across all notebooks for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "angry-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "months = ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct']\n",
    "n_start = 0\n",
    "n_end = 80 #40\n",
    "# number of months * number of examples from each month (n_end  - n_start); n_end excluded\n",
    "model_suffix = len(months) * (n_end - n_start)\n",
    "sub_group = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-dimension",
   "metadata": {},
   "source": [
    "### Annotated files generated from Doccano & rehearsal data notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "technical-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET this flag to False if not using SYMPTOM as part of NER model\n",
    "keep_symptom_ent = False\n",
    "\n",
    "if not keep_symptom_ent:\n",
    "    model_suffix = str(model_suffix) + '_NS_v2'\n",
    "\n",
    "# final annotated output from Doccano\n",
    "train_filename = './data/json/nsamples_480_v2_2021_6m_doccano.jsonl' #'./nsamples_240_2021_6m_doccano.jsonl'\n",
    "val_filename = './data/json/val_nsamples_240_doccano.jsonl'\n",
    "\n",
    "# annotated rehearsal filenames from generate_rehearsal_data.ipynb\n",
    "rehearsal_train_filename = './data/json/nlp_rehearsal_1000.json'\n",
    "rehearsal_val_filename = './data/json/test_nlp_rehearsal_1000.json'\n",
    "\n",
    "old_ptrns_fname = f'./data/patterns/old_patterns_240.csv' #f'./old_patterns_60.csv'\n",
    "new_ptrns_fname = f'./data/patterns/old_patterns_{model_suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-deficit",
   "metadata": {},
   "source": [
    "### binary data sets generated from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "short-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "docbin_train = f'./outputs/train_n_{model_suffix}.spacy'\n",
    "docbin_val = f'./outputs/val_n_240_NS.spacy' #f'./outputs/val_n_{model_suffix}.spacy'\n",
    "docbin_rhrsl_train = f'./outputs/rhrsl_train_n_240_NS.spacy' #f'./outputs/rhrsl_train_n_{model_suffix}.spacy'\n",
    "docbin_rhrsl_val = f'./outputs/rhrsl_val_n_240_NS.spacy' #f'./outputs/rhrsl_val_n_{model_suffix}.spacy'\n",
    "\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "different-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, labels = get_list_from_docbin(docbin_train)\n",
    "val_data, _ = get_list_from_docbin(docbin_val)\n",
    "\n",
    "rhrsl_train_data, _ = get_list_from_docbin(docbin_rhrsl_train)\n",
    "rhrsl_val_data, _ = get_list_from_docbin(docbin_rhrsl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-trailer",
   "metadata": {},
   "source": [
    "## MODEL 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-heath",
   "metadata": {},
   "source": [
    "### Model training / fine-tuning using minibatch, pre-trained model: pseudo rehearsal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-source",
   "metadata": {},
   "source": [
    "This model is trained using the pre-trained model from spacy. To avoid the problem of catastrophic forgetting, this model uses rehearsal data in addition to the train dataset. Rehearsal data is entity data generated using the default pre-trained spacy model. Using such examples in the training set helps the model 'remember' to predict the entities it was originally trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "equipped-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune = spacy.load('en_core_web_sm') #Model 1\n",
    "\n",
    "# get ner component from pipeline\n",
    "ner = nlp_fine_tune.get_pipe('ner')\n",
    "\n",
    "#Add custom labels to NER component\n",
    "for l in labels:\n",
    "    ner.add_label(l)\n",
    "\n",
    "#check which labels are present in rehearsal train data\n",
    "labels_rehearsal = set()\n",
    "for _, annotations in rhrsl_train_data:\n",
    "    for ent in annotations['entities']:\n",
    "        labels_rehearsal.add(ent[2])\n",
    "labels_rehearsal;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ruled-companion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses at iteration 0 - 2021-12-20 02:11:12.363157: {'ner': 5576.467792162419}\n",
      "losses at iteration 1 - 2021-12-20 02:11:44.061580: {'ner': 4332.883394004506}\n",
      "losses at iteration 2 - 2021-12-20 02:12:15.859403: {'ner': 4047.6502895367644}\n",
      "losses at iteration 3 - 2021-12-20 02:12:47.976512: {'ner': 3189.793058603752}\n",
      "losses at iteration 4 - 2021-12-20 02:13:20.359124: {'ner': 2636.6828106756566}\n",
      "losses at iteration 5 - 2021-12-20 02:13:52.581651: {'ner': 2381.0459341477003}\n",
      "losses at iteration 6 - 2021-12-20 02:14:24.672039: {'ner': 1959.2390458196207}\n",
      "losses at iteration 7 - 2021-12-20 02:14:56.631376: {'ner': 1785.542078595968}\n",
      "losses at iteration 8 - 2021-12-20 02:15:28.191288: {'ner': 1511.8160156939323}\n",
      "losses at iteration 9 - 2021-12-20 02:15:59.506220: {'ner': 1338.808810117102}\n",
      "losses at iteration 10 - 2021-12-20 02:16:31.393060: {'ner': 1271.8543044296684}\n",
      "losses at iteration 11 - 2021-12-20 02:17:02.661805: {'ner': 1128.3592588404174}\n",
      "losses at iteration 12 - 2021-12-20 02:17:34.029247: {'ner': 1027.7568259247012}\n",
      "losses at iteration 13 - 2021-12-20 02:18:05.632416: {'ner': 938.4508841056567}\n",
      "losses at iteration 14 - 2021-12-20 02:18:36.910462: {'ner': 871.4694473306774}\n",
      "losses at iteration 15 - 2021-12-20 02:19:08.445071: {'ner': 793.5736550016278}\n",
      "losses at iteration 16 - 2021-12-20 02:19:40.368587: {'ner': 755.9985195329926}\n",
      "losses at iteration 17 - 2021-12-20 02:20:12.330996: {'ner': 727.5091587787225}\n",
      "losses at iteration 18 - 2021-12-20 02:20:44.137554: {'ner': 646.8965426144948}\n",
      "losses at iteration 19 - 2021-12-20 02:21:15.915822: {'ner': 657.5342443142915}\n",
      "losses at iteration 20 - 2021-12-20 02:21:47.968520: {'ner': 580.3956167699006}\n",
      "losses at iteration 21 - 2021-12-20 02:22:19.499842: {'ner': 515.3919882710952}\n",
      "losses at iteration 22 - 2021-12-20 02:22:51.367447: {'ner': 555.1673256284922}\n",
      "losses at iteration 23 - 2021-12-20 02:23:23.029057: {'ner': 497.6833289744842}\n",
      "losses at iteration 24 - 2021-12-20 02:23:54.727400: {'ner': 493.9518254601621}\n",
      "losses at iteration 25 - 2021-12-20 02:24:26.767588: {'ner': 430.16810091282116}\n",
      "losses at iteration 26 - 2021-12-20 02:24:58.816623: {'ner': 419.74530586420997}\n",
      "losses at iteration 27 - 2021-12-20 02:25:30.667553: {'ner': 446.9977404715358}\n",
      "losses at iteration 28 - 2021-12-20 02:26:02.046331: {'ner': 439.79332110433614}\n",
      "losses at iteration 29 - 2021-12-20 02:26:34.281179: {'ner': 392.8743641388896}\n",
      "losses at iteration 30 - 2021-12-20 02:27:06.002440: {'ner': 435.9590891978857}\n",
      "losses at iteration 31 - 2021-12-20 02:27:37.316606: {'ner': 362.09474736990467}\n",
      "losses at iteration 32 - 2021-12-20 02:28:09.229148: {'ner': 363.4813505814177}\n",
      "losses at iteration 33 - 2021-12-20 02:28:40.618495: {'ner': 314.85098490594174}\n",
      "losses at iteration 34 - 2021-12-20 02:29:12.166389: {'ner': 369.06942530771136}\n",
      "losses at iteration 35 - 2021-12-20 02:29:44.111701: {'ner': 278.35737420055347}\n",
      "losses at iteration 36 - 2021-12-20 02:30:16.174011: {'ner': 329.18335424205026}\n",
      "losses at iteration 37 - 2021-12-20 02:30:48.154423: {'ner': 332.97045652296356}\n",
      "losses at iteration 38 - 2021-12-20 02:31:19.863567: {'ner': 268.34546764985436}\n",
      "losses at iteration 39 - 2021-12-20 02:31:51.319141: {'ner': 313.51582389679743}\n"
     ]
    }
   ],
   "source": [
    "# Get names of other pipes to disable them during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp_fine_tune.pipe_names if pipe != 'ner']\n",
    "\n",
    "optimizer_ft = nlp_fine_tune.resume_training()\n",
    "# optimizer_ft.learn_rate = 0.0005\n",
    "combined_data = train_data + rhrsl_train_data\n",
    "with nlp_fine_tune.disable_pipes(*other_pipes):\n",
    "    for i in range(epochs):\n",
    "        losses = {}\n",
    "        np.random.shuffle(combined_data)\n",
    "        batches = minibatch(combined_data, size=compounding(4.0, 16.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            texts, annotations = zip(*batch)\n",
    "            for j in range(len(texts)):\n",
    "                # create Example\n",
    "                doc = nlp_fine_tune.make_doc(texts[j])\n",
    "                example = Example.from_dict(doc, {\"entities\": annotations[j]['entities']})\n",
    "                examples.append(example)\n",
    "            \n",
    "            # Update the model\n",
    "            nlp_fine_tune.update(examples, sgd=optimizer_ft, drop=0.2, losses=losses)\n",
    "        print (f'losses at iteration {i} - {dt.datetime.now()}: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "affiliated-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune.to_disk(f'./models/model_1_n_{model_suffix}_LR_0005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "blank-house",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_ft.learn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "tamil-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save combined dataset used for training the model: rehearsal data + train data\n",
    "# fine_tune_entities_dct = convert_to_spacy_BILOU_format(combined_data, 'fine_tune_tmp1_1000_spacy_format.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-lover",
   "metadata": {},
   "source": [
    "## MODEL 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-extension",
   "metadata": {},
   "source": [
    "### Model training / fine-tuning using minibatch, pre-trained model: no rehearsal, 1 epoch ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "entitled-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune_no_rhrsl = spacy.load('en_core_web_sm') #Model 2\n",
    "\n",
    "# get ner component from pipeline\n",
    "ner = nlp_fine_tune_no_rhrsl.get_pipe('ner')\n",
    "\n",
    "#Add custom labels to NER component\n",
    "for l in labels:\n",
    "    ner.add_label(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aboriginal-injury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses at iteration 0 - 2021-12-16 22:23:57.377006: {'ner': 3081.422375679847}\n"
     ]
    }
   ],
   "source": [
    "# Get names of other pipes to disable them during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp_fine_tune_no_rhrsl.pipe_names if pipe != 'ner']\n",
    "\n",
    "optimizer_ft = nlp_fine_tune_no_rhrsl.resume_training()\n",
    "with nlp_fine_tune_no_rhrsl.disable_pipes(*other_pipes):\n",
    "    for i in range(1):\n",
    "        losses = {}\n",
    "        np.random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 16.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            texts, annotations = zip(*batch)\n",
    "            for j in range(len(texts)):\n",
    "                # create Example\n",
    "                doc = nlp_fine_tune_no_rhrsl.make_doc(texts[j])\n",
    "                example = Example.from_dict(doc, {\"entities\": annotations[j]['entities']})\n",
    "                examples.append(example)\n",
    "            \n",
    "            # Update the model\n",
    "            nlp_fine_tune_no_rhrsl.update(examples, sgd=optimizer_ft, drop=0.2, losses=losses)\n",
    "        print (f'losses at iteration {i} - {dt.datetime.now()}: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "adjacent-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune_no_rhrsl.to_disk(f'./models/model_2_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-schedule",
   "metadata": {},
   "source": [
    "## MODEL 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-warrant",
   "metadata": {},
   "source": [
    "### Model training using minibatch, Blank spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "reduced-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank = spacy.blank('en') # Model 3\n",
    "\n",
    "nlp_blank.add_pipe('ner', last=True)\n",
    "ner = nlp_blank.get_pipe('ner')\n",
    "#Add custom labels to NER component\n",
    "for l in labels:\n",
    "    ner.add_label(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "black-currency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses at iteration 0 - 2021-12-20 01:58:31.025332: {'ner': 8646.016031546464}\n",
      "losses at iteration 1 - 2021-12-20 01:58:45.443145: {'ner': 1747.1857977169348}\n",
      "losses at iteration 2 - 2021-12-20 01:58:59.793075: {'ner': 1483.7506279624583}\n",
      "losses at iteration 3 - 2021-12-20 01:59:14.321139: {'ner': 1267.7645954702518}\n",
      "losses at iteration 4 - 2021-12-20 01:59:28.629892: {'ner': 797.0254093600046}\n",
      "losses at iteration 5 - 2021-12-20 01:59:43.143051: {'ner': 640.9149698029494}\n",
      "losses at iteration 6 - 2021-12-20 01:59:57.643876: {'ner': 502.67452125092115}\n",
      "losses at iteration 7 - 2021-12-20 02:00:12.395149: {'ner': 479.7778080427033}\n",
      "losses at iteration 8 - 2021-12-20 02:00:27.097337: {'ner': 408.4687196867791}\n",
      "losses at iteration 9 - 2021-12-20 02:00:41.792577: {'ner': 348.01954025892303}\n",
      "losses at iteration 10 - 2021-12-20 02:00:56.446528: {'ner': 318.60855383865794}\n",
      "losses at iteration 11 - 2021-12-20 02:01:11.091614: {'ner': 246.3675319984598}\n",
      "losses at iteration 12 - 2021-12-20 02:01:25.839707: {'ner': 240.62036066722948}\n",
      "losses at iteration 13 - 2021-12-20 02:01:40.229987: {'ner': 247.84576112502901}\n",
      "losses at iteration 14 - 2021-12-20 02:01:55.496352: {'ner': 239.41951078369092}\n",
      "losses at iteration 15 - 2021-12-20 02:02:10.001587: {'ner': 230.8781030468794}\n",
      "losses at iteration 16 - 2021-12-20 02:02:24.449835: {'ner': 229.89627207756885}\n",
      "losses at iteration 17 - 2021-12-20 02:02:38.947143: {'ner': 199.0404357498835}\n",
      "losses at iteration 18 - 2021-12-20 02:02:53.295113: {'ner': 175.46474237032746}\n",
      "losses at iteration 19 - 2021-12-20 02:03:07.460212: {'ner': 167.58836942048262}\n",
      "losses at iteration 20 - 2021-12-20 02:03:21.582367: {'ner': 180.10091688760423}\n",
      "losses at iteration 21 - 2021-12-20 02:03:36.006340: {'ner': 172.59801759093082}\n",
      "losses at iteration 22 - 2021-12-20 02:03:50.436667: {'ner': 134.70414126089943}\n",
      "losses at iteration 23 - 2021-12-20 02:04:05.166308: {'ner': 146.39677040631045}\n",
      "losses at iteration 24 - 2021-12-20 02:04:19.655324: {'ner': 144.79131130490785}\n",
      "losses at iteration 25 - 2021-12-20 02:04:34.375226: {'ner': 121.43959208498858}\n",
      "losses at iteration 26 - 2021-12-20 02:04:48.705415: {'ner': 129.02953364372416}\n",
      "losses at iteration 27 - 2021-12-20 02:05:02.927958: {'ner': 123.59382533931472}\n",
      "losses at iteration 28 - 2021-12-20 02:05:17.411184: {'ner': 102.5859132415773}\n",
      "losses at iteration 29 - 2021-12-20 02:05:31.762282: {'ner': 90.52910429745923}\n",
      "losses at iteration 30 - 2021-12-20 02:05:46.428453: {'ner': 117.80070908050995}\n",
      "losses at iteration 31 - 2021-12-20 02:06:00.469161: {'ner': 119.73148991728424}\n",
      "losses at iteration 32 - 2021-12-20 02:06:15.101901: {'ner': 102.7677463233632}\n",
      "losses at iteration 33 - 2021-12-20 02:06:29.596535: {'ner': 82.91636944907148}\n",
      "losses at iteration 34 - 2021-12-20 02:06:44.351958: {'ner': 67.47075373644817}\n",
      "losses at iteration 35 - 2021-12-20 02:06:58.672006: {'ner': 81.08997477804957}\n",
      "losses at iteration 36 - 2021-12-20 02:07:13.284986: {'ner': 79.64931604189943}\n",
      "losses at iteration 37 - 2021-12-20 02:07:27.674028: {'ner': 76.14516818913849}\n",
      "losses at iteration 38 - 2021-12-20 02:07:42.034353: {'ner': 74.64301845988085}\n",
      "losses at iteration 39 - 2021-12-20 02:07:56.586335: {'ner': 81.6290007699935}\n"
     ]
    }
   ],
   "source": [
    "# Get names of other pipes to disable them during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp_blank.pipe_names if pipe != 'ner']\n",
    "\n",
    "optimizer_blnk = nlp_blank.begin_training()\n",
    "\n",
    "with nlp_blank.disable_pipes(*other_pipes):\n",
    "    for i in range(epochs):\n",
    "        losses = {}\n",
    "        np.random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 16.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            texts, annotations = zip(*batch)\n",
    "            for j in range(len(texts)):\n",
    "                # create Example\n",
    "                doc = nlp_blank.make_doc(texts[j])\n",
    "                example = Example.from_dict(doc, {\"entities\": annotations[j]['entities']})\n",
    "                examples.append(example)\n",
    "            \n",
    "            # Update the model\n",
    "            nlp_blank.update(examples, sgd=optimizer_blnk, drop=0.2, losses=losses)\n",
    "        print (f'losses at iteration {i} - {dt.datetime.now()}: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "funny-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank.to_disk(f'./models/model_3_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-immunology",
   "metadata": {},
   "source": [
    "## MODEL 6: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-qualification",
   "metadata": {},
   "source": [
    "### Model training pre-trained model: use only rehearsal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cathedral-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune_only_rhrsl = spacy.load('en_core_web_sm') #Model 2\n",
    "\n",
    "# get ner component from pipeline\n",
    "ner = nlp_fine_tune_only_rhrsl.get_pipe('ner')\n",
    "\n",
    "#Add custom labels to NER component\n",
    "for l in labels:\n",
    "    ner.add_label(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "small-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses at iteration 0 - 2021-12-16 23:15:34.427482: {'ner': 1745.24130276536}\n",
      "losses at iteration 1 - 2021-12-16 23:15:48.032187: {'ner': 1347.4078114786373}\n",
      "losses at iteration 2 - 2021-12-16 23:16:01.632819: {'ner': 1042.8760053135156}\n",
      "losses at iteration 3 - 2021-12-16 23:16:15.189968: {'ner': 850.890962165866}\n",
      "losses at iteration 4 - 2021-12-16 23:16:28.906513: {'ner': 768.8895616634314}\n",
      "losses at iteration 5 - 2021-12-16 23:16:42.685366: {'ner': 643.108800899378}\n",
      "losses at iteration 6 - 2021-12-16 23:16:56.334516: {'ner': 536.0692065412068}\n",
      "losses at iteration 7 - 2021-12-16 23:17:10.040106: {'ner': 539.9952681452605}\n",
      "losses at iteration 8 - 2021-12-16 23:17:23.692236: {'ner': 474.26512351170066}\n",
      "losses at iteration 9 - 2021-12-16 23:17:37.513360: {'ner': 392.3544184032194}\n",
      "losses at iteration 10 - 2021-12-16 23:17:51.228502: {'ner': 384.8074740658636}\n",
      "losses at iteration 11 - 2021-12-16 23:18:04.854083: {'ner': 344.8514528440462}\n",
      "losses at iteration 12 - 2021-12-16 23:18:18.517134: {'ner': 325.21202970789375}\n",
      "losses at iteration 13 - 2021-12-16 23:18:32.125160: {'ner': 300.35953828818117}\n",
      "losses at iteration 14 - 2021-12-16 23:18:45.723818: {'ner': 293.53105361861145}\n",
      "losses at iteration 15 - 2021-12-16 23:18:59.518286: {'ner': 273.26913915355556}\n",
      "losses at iteration 16 - 2021-12-16 23:19:13.129513: {'ner': 275.8038531734344}\n",
      "losses at iteration 17 - 2021-12-16 23:19:26.718918: {'ner': 251.38891484915345}\n",
      "losses at iteration 18 - 2021-12-16 23:19:40.282864: {'ner': 244.47948167335073}\n",
      "losses at iteration 19 - 2021-12-16 23:19:53.847946: {'ner': 273.7052452853956}\n",
      "losses at iteration 20 - 2021-12-16 23:20:07.468293: {'ner': 295.1190813148637}\n",
      "losses at iteration 21 - 2021-12-16 23:20:21.102597: {'ner': 256.7811664118963}\n",
      "losses at iteration 22 - 2021-12-16 23:20:34.733237: {'ner': 190.58880234771658}\n",
      "losses at iteration 23 - 2021-12-16 23:20:48.349797: {'ner': 200.42758518880615}\n",
      "losses at iteration 24 - 2021-12-16 23:21:02.072740: {'ner': 187.5788985569701}\n",
      "losses at iteration 25 - 2021-12-16 23:21:15.696913: {'ner': 192.55366958618606}\n",
      "losses at iteration 26 - 2021-12-16 23:21:29.317121: {'ner': 208.81230127097834}\n",
      "losses at iteration 27 - 2021-12-16 23:21:42.907492: {'ner': 196.57244168097307}\n",
      "losses at iteration 28 - 2021-12-16 23:21:56.627231: {'ner': 172.32877533672325}\n",
      "losses at iteration 29 - 2021-12-16 23:22:10.267948: {'ner': 182.41751523122554}\n",
      "losses at iteration 30 - 2021-12-16 23:22:23.918543: {'ner': 168.59646927904393}\n",
      "losses at iteration 31 - 2021-12-16 23:22:37.542071: {'ner': 163.50145151400875}\n",
      "losses at iteration 32 - 2021-12-16 23:22:51.239901: {'ner': 189.11904274053094}\n",
      "losses at iteration 33 - 2021-12-16 23:23:05.066445: {'ner': 155.43095197669}\n",
      "losses at iteration 34 - 2021-12-16 23:23:18.784161: {'ner': 116.02155652772969}\n",
      "losses at iteration 35 - 2021-12-16 23:23:32.502967: {'ner': 171.37325215755297}\n",
      "losses at iteration 36 - 2021-12-16 23:23:46.419450: {'ner': 133.15722894274657}\n",
      "losses at iteration 37 - 2021-12-16 23:24:00.359800: {'ner': 164.78360967822533}\n",
      "losses at iteration 38 - 2021-12-16 23:24:14.317153: {'ner': 160.60458962208145}\n",
      "losses at iteration 39 - 2021-12-16 23:24:28.063160: {'ner': 172.57253743340067}\n"
     ]
    }
   ],
   "source": [
    "# Get names of other pipes to disable them during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp_fine_tune_only_rhrsl.pipe_names if pipe != 'ner']\n",
    "\n",
    "optimizer_ft = nlp_fine_tune_only_rhrsl.resume_training()\n",
    "with nlp_fine_tune_only_rhrsl.disable_pipes(*other_pipes):\n",
    "    for i in range(epochs):\n",
    "        losses = {}\n",
    "        np.random.shuffle(rhrsl_train_data)\n",
    "        batches = minibatch(rhrsl_train_data, size=compounding(4.0, 16.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            texts, annotations = zip(*batch)\n",
    "            for j in range(len(texts)):\n",
    "                # create Example\n",
    "                doc = nlp_fine_tune_only_rhrsl.make_doc(texts[j])\n",
    "                example = Example.from_dict(doc, {\"entities\": annotations[j]['entities']})\n",
    "                examples.append(example)\n",
    "            \n",
    "            # Update the model\n",
    "            nlp_fine_tune_only_rhrsl.update(examples, sgd=optimizer_ft, drop=0.2, losses=losses)\n",
    "        print (f'losses at iteration {i} - {dt.datetime.now()}: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "apparent-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fine_tune_only_rhrsl.to_disk(f'./models/model_6_n_{model_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-vertex",
   "metadata": {},
   "source": [
    "### Model training using single examples, Blank spacy model: COMMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "present-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_b = spacy.blank('en')\n",
    "\n",
    "# nlp_b.add_pipe('ner', last=True)\n",
    "# ner = nlp_b.get_pipe('ner')\n",
    "# #Add custom labels to NER component\n",
    "# for l in labels:\n",
    "#     ner.add_label(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "exceptional-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_b = nlp_b.begin_training()\n",
    "\n",
    "# for i in range(1):\n",
    "#     losses = {}\n",
    "#     np.random.shuffle(train_data)\n",
    "#     for raw_text, entity_offsets in train_data:\n",
    "#         doc = nlp_b.make_doc(raw_text)\n",
    "#         example = Example.from_dict(doc, {\"entities\": entity_offsets['entities']})\n",
    "#         nlp_b.update([example], sgd=optimizer_b, losses=losses)\n",
    "#     print (f'losses at iteration {i} - {dt.datetime.now()}: {losses}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
