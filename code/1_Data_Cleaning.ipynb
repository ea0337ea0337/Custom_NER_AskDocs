{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peaceful-operations",
   "metadata": {},
   "source": [
    "### This notebook cleans text data from .csv files and exports it as .jsonl files for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "corresponding-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# other modules, code\n",
    "from html import unescape\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "import spacy\n",
    "import srsly\n",
    "from spacy import displacy\n",
    "from spacy.training import docs_to_json, offsets_to_biluo_tags, biluo_tags_to_spans\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokens import Span\n",
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "shaped-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df0):\n",
    "    '''\n",
    "    Clean original dataframes, drop null values and merge title, remove posts marked deleted etc.\n",
    "    '''\n",
    "    df = df0.copy() #BAD CODE since creating a copy inside the function. Fix it!!!!!\n",
    "    \n",
    "    # fill nan with empty string\n",
    "    df['selftext'] = df['selftext'].fillna('')\n",
    "    df['flair_text'] = df['flair_text'].fillna('no_flair')\n",
    "    \n",
    "    # drop rows where selftext was deleted or removed\n",
    "    df = df[~((df['selftext'] == '[deleted]') | (df['selftext'] == '[removed]'))]\n",
    "    df['text'] = df['title'] + ' ' + df['selftext']\n",
    "    \n",
    "    #reset the index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns = 'index', inplace=True)    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yellow-upset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def remove_accented_chars(text):\n",
    "    '''\n",
    "    Remove accents from characters\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "remove_accented_chars('naïve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lucky-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken and modified from https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n",
    "def standardize_text(df, text_field):\n",
    "    '''\n",
    "    Clean model text using regular expressions\n",
    "    '''\n",
    "    #convert to lowercase\n",
    "    #df[text_field] = df[text_field].str.lower()\n",
    "    \n",
    "    #remove accented characters\n",
    "    df[text_field] = df[text_field].map(remove_accented_chars)\n",
    "    \n",
    "    #unesscape html characters\n",
    "    df[text_field] = df[text_field].map(unescape)\n",
    "    \n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"www\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\n\\n+\", \"\\n\", regex=True)\n",
    "    \n",
    "    df[text_field] = df[text_field].str.replace(r\"[‘’]\", \"'\", regex=True)\n",
    "\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),.!?@'`\\\"_\\n]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    \n",
    "    # expand some acronyms\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\bdae\\b\", \"does anyone else\", regex=True)\n",
    "    ## TODO: NEED TO INCLUDE doctor terms in the text; Should help with model performance.\n",
    "    ## df[text_field] = df[text_field].str.replace(r\"\\b(doctor|doc|dr|dr\\.)\\b\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\bgad\\b\", \"generalized anxiety disorder\", regex=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "normal-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ranks(grp):\n",
    "    n_rows = grp.shape[0]\n",
    "    grp[\"rank\"] = grp[\"rnd\"].rank()\n",
    "    grp[\"trt\"] = (grp[\"rank\"] - 1).astype(int) // (n_rows / 2)\n",
    "    grp[\"trt\"] = grp[\"trt\"].map({0: 'train', 1: 'test'})\n",
    "    return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unexpected-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clean_text(csv_file):\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    df.rename(columns = {'author_flair_text': 'flair_text'}, inplace=True)\n",
    "    df = clean_df(df)\n",
    "    df = standardize_text(df, 'text')\n",
    "    df = df[['text']]\n",
    "    clean_file = csv_file.split('.csv')[0] +'_clean.csv'\n",
    "    df.to_csv(clean_file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-sending",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fundamental-rugby",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = './Data/AskDocs_Oct_2020.csv'\n",
    "# df = pd.read_csv(filename)\n",
    "# df.rename(columns = {'author_flair_text': 'flair_text'}, inplace=True)\n",
    "# df = clean_df(df)\n",
    "# df['text'].loc[0]\n",
    "# # df = create_clean_text(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "requested-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = standardize_text(df, 'text')\n",
    "# df['text'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lyric-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_df(months):\n",
    "    df_list = []\n",
    "    for month in months:\n",
    "        df = pd.read_csv('./data/askdocs_csv/AskDocs_' + month + '_2021_clean.csv')\n",
    "        df['month'] = month\n",
    "        df_list.append(df)\n",
    "    df_combined = pd.concat(df_list)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "distinguished-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_df(df_combined, grp_name, n_start, n_stop, months):\n",
    "    df_grp = []\n",
    "    for month in months:\n",
    "        df = df_combined[(df_combined['month'] == month) & (df_combined['trt'] == grp_name)][n_start:n_stop]\n",
    "        df_grp.append(df)\n",
    "    df_grp = pd.concat(df_grp)\n",
    "    return df_grp[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-sheffield",
   "metadata": {},
   "source": [
    "### Initialize basic arguments, Clean Original dataset and save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "heavy-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "months = ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct']\n",
    "n_start = 40\n",
    "n_end = 80 #40\n",
    "# number of months * number of examples from each month (n_end  - n_start); n_end excluded\n",
    "model_suffix = len(months) * (n_end - n_start)\n",
    "sub_group = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-application",
   "metadata": {},
   "source": [
    "### Annotated files generated from Doccano & rehearsal data notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "central-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET this flag to False if not using SYMPTOM as part of NER model\n",
    "keep_symptom_ent = False\n",
    "\n",
    "if not keep_symptom_ent:\n",
    "    model_suffix = str(model_suffix) + '_NS_v2'\n",
    "\n",
    "# final annotated output from Doccano\n",
    "train_filename = './data/json/nsamples_480_v2_2021_6m_doccano.jsonl' #'./nsamples_240_2021_6m_doccano.jsonl'\n",
    "val_filename = './data/json/val_nsamples_240_doccano.jsonl'\n",
    "\n",
    "# annotated rehearsal filenames from generate_rehearsal_data.ipynb\n",
    "rehearsal_train_filename = './data/json/nlp_rehearsal_1000.json'\n",
    "rehearsal_val_filename = './data/json/test_nlp_rehearsal_1000.json'\n",
    "\n",
    "old_ptrns_fname = f'./data/patterns/old_patterns_240.csv' #f'./old_patterns_60.csv'\n",
    "new_ptrns_fname = f'./data/patterns/old_patterns_{model_suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-garden",
   "metadata": {},
   "source": [
    "### Read in original csv files, clean, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "forbidden-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    df = create_clean_text(f'./data/askdocs_csv/AskDocs_{month}_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-chester",
   "metadata": {},
   "source": [
    "### Read in cleaned csv files, concatenate into single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "royal-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82680, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = create_combined_df(months)\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-debut",
   "metadata": {},
   "source": [
    "### Shuffle randomly, split each month into 2 parts: train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "determined-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['rnd'] = np.random.rand(df_combined.shape[0])\n",
    "df_combined = df_combined.groupby(\"month\").apply(assign_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-qatar",
   "metadata": {},
   "source": [
    "### Select data within subgroup (train / test) from combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "verified-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_text = select_from_df(df_combined, sub_group, n_start, n_end, months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acknowledged-cornell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-brother",
   "metadata": {},
   "source": [
    "### IF NOT PRE-ANNOTATING: Write cleaned text to .JSONL for import into Doccano for annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "funded-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data as jsonl file for import into doccano for annotation\n",
    "df_clean_text.to_json(f'./outputs/{sub_group}_n_{model_suffix}.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-tribune",
   "metadata": {},
   "source": [
    "## Pre-Annotate text using existing patterns before importing into Doccano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-findings",
   "metadata": {},
   "source": [
    "### Use existing identified patterns (MODEL 0) to pre-annotate data before importing in Doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "scheduled-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entity_ruler model created using patterns from all the training data so far\n",
    "model_0 = spacy.load('./models/model_0_n_480_NS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "common-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate pre-annotated test data based on existing annotation rules\n",
    "pre_annotated_eg = []\n",
    "for eg in df_clean_text['text']:\n",
    "    dct = {}\n",
    "    doc = model_0(eg)\n",
    "    dct['text'] = doc.text\n",
    "    l = []\n",
    "    if (len(doc.ents)):\n",
    "        for ent in doc.ents:\n",
    "            #print (ent, ent.start_char, ent.end_char, ent.label_)\n",
    "            l.append([ent.start_char, ent.end_char, ent.label_])\n",
    "    dct['label'] = l\n",
    "    pre_annotated_eg.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "frozen-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "pre_annotated_eg_df = pd.DataFrame(pre_annotated_eg)\n",
    "# save in json format for import into doccano\n",
    "pre_annotated_eg_df.to_json(f'./outputs/pre_annotated_val_240_480.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-jewelry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
